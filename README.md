# MasterThesisProject

It is software and embedded field.

"Embedded aplication device, determining piano sound color using AI."

AI recognizes 3 sound colors:
 - dark
 - bright
 - generic

Needed libraries for train and data sorting & split: 

pip install pyroomacoustics tensorflow librosa numpy keras matplotlib audiomentations scikit-learn pandas tensorflow-probability tf-keras  joblib 

NSynth dataset has been used.

Repository provides code for preparing and training model inf folder AI (my tflite models included) and RasPi folder, for code and operations needed to run application on Easpberry Pi. Data sorters are set to find acoustic and synthetic keyboard sounds. It's based on database JSON files, indexes described on Magenta TensorFlow:

**> https://magenta.tensorflow.org/datasets/nsynth#files <**

official website provides descryption of JSON files **Index ID refers to column in "qualities" section**. The labels are binary, 0 means that this characteristic is absent, and 1 means that it is present. Repository contains two directories. First contains code for ML preparation, second one describes how to set enviroment and application, including model deployment, on Raspberry Pi (in my case model 4B, Bookworm OS). 

ML determines sound color based on analysis of MEL-Spectrogram and Chromagram. Data is generated by creating graphs of augmented (possible by using audiomentation library) acoustic and synthetic keyboard .wav files.

Used neural net: CNN

Data is processed by image recognition.

For Raspberry application, we need to have python installed. Needed libraries:

pip3 install tensorflow librosa sounddevice matplotlib pillow

Graphic interface will be written using Kivy:  https://kivy.org/doc/stable/installation/installation-rpi.html , https://kivy.org/doc/stable/gettingstarted/installation.html#installation-canonical 

Animations will be made with Synfig.



